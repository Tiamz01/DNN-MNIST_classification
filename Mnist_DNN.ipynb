{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce4d720",
   "metadata": {},
   "source": [
    "#### import the relevant package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "561c1bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\py310-TF\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd4545b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist_datasets, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "mnist_train, mnist_test = mnist_datasets['train'], mnist_datasets['test']\n",
    "\n",
    "# Calculate the number of validation samples and cast to int64\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "# Define a function to scale images\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "# Apply scaling to training and validation data\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# Shuffle the scaled training and validation data\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# Create validation data by taking a portion of the shuffled data\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# Create validation data by taking a portion of the shuffled data and apply reshaping\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "validation_data = validation_data.map(lambda x, y: (tf.reshape(x, shape=(-1, 784)), y))\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# # Batch the training data\n",
    "# train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "# # Batch the validation data\n",
    "# validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "# # Batch the test data\n",
    "# test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "# # Get validation inputs and targets\n",
    "# validation_inputs, validation_targets = next(iter(validation_data))\n",
    "\n",
    "# Batch the training data and reshape it\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "train_data = train_data.map(lambda x, y: (tf.reshape(x, shape=(-1, 784)), y))\n",
    "\n",
    "# Batch the validation data and reshape it\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "validation_inputs = tf.reshape(validation_inputs, shape=(-1, 784))\n",
    "\n",
    "# Batch the test data and reshape it\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "test_inputs, test_targets = next(iter(test_data))\n",
    "test_inputs = tf.reshape(test_inputs, shape=(-1, 784))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a587d7",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "#### Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9795618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 70\n",
    "\n",
    "# Create a Sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(input_size,)),  # Input layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),  # Hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),  # Hidden layer\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')  # Output layer (returns probabilities)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325c835c",
   "metadata": {},
   "source": [
    "### choose the optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cf6e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb102454",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f3b311",
   "metadata": {},
   "source": [
    "#### WHAT HAPPENS INSIDE AN EPOCH\n",
    "\n",
    "1. At the beginning of each epoch, the training loss will be set to 0 \n",
    "2. The algorithm will iterate over a preset number of batches, all from train_data\n",
    "3. The weights and biases will be updated as many times as there are batches\n",
    "4. We will get a value for the loss function, indicating how the training is going\n",
    "5. We will also see a training accuracy \n",
    "6. At the end of the epoch, the algorithm will forward propagate the whole validation set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7815ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "540/540 - 7s - loss: 0.3601 - accuracy: 0.8991 - val_loss: 0.1939 - val_accuracy: 0.9435 - 7s/epoch - 12ms/step\n",
      "Epoch 2/15\n",
      "540/540 - 4s - loss: 0.1593 - accuracy: 0.9532 - val_loss: 0.1352 - val_accuracy: 0.9610 - 4s/epoch - 8ms/step\n",
      "Epoch 3/15\n",
      "540/540 - 6s - loss: 0.1159 - accuracy: 0.9655 - val_loss: 0.1201 - val_accuracy: 0.9665 - 6s/epoch - 11ms/step\n",
      "Epoch 4/15\n",
      "540/540 - 6s - loss: 0.0927 - accuracy: 0.9723 - val_loss: 0.1029 - val_accuracy: 0.9695 - 6s/epoch - 11ms/step\n",
      "Epoch 5/15\n",
      "540/540 - 6s - loss: 0.0768 - accuracy: 0.9769 - val_loss: 0.0856 - val_accuracy: 0.9733 - 6s/epoch - 11ms/step\n",
      "Epoch 6/15\n",
      "540/540 - 6s - loss: 0.0637 - accuracy: 0.9803 - val_loss: 0.0752 - val_accuracy: 0.9768 - 6s/epoch - 10ms/step\n",
      "Epoch 7/15\n",
      "540/540 - 5s - loss: 0.0546 - accuracy: 0.9843 - val_loss: 0.0627 - val_accuracy: 0.9797 - 5s/epoch - 10ms/step\n",
      "Epoch 8/15\n",
      "540/540 - 5s - loss: 0.0464 - accuracy: 0.9859 - val_loss: 0.0534 - val_accuracy: 0.9828 - 5s/epoch - 10ms/step\n",
      "Epoch 9/15\n",
      "540/540 - 5s - loss: 0.0406 - accuracy: 0.9878 - val_loss: 0.0527 - val_accuracy: 0.9845 - 5s/epoch - 10ms/step\n",
      "Epoch 10/15\n",
      "540/540 - 5s - loss: 0.0353 - accuracy: 0.9890 - val_loss: 0.0508 - val_accuracy: 0.9835 - 5s/epoch - 10ms/step\n",
      "Epoch 11/15\n",
      "540/540 - 6s - loss: 0.0312 - accuracy: 0.9904 - val_loss: 0.0416 - val_accuracy: 0.9863 - 6s/epoch - 11ms/step\n",
      "Epoch 12/15\n",
      "540/540 - 6s - loss: 0.0296 - accuracy: 0.9911 - val_loss: 0.0299 - val_accuracy: 0.9907 - 6s/epoch - 10ms/step\n",
      "Epoch 13/15\n",
      "540/540 - 5s - loss: 0.0244 - accuracy: 0.9924 - val_loss: 0.0452 - val_accuracy: 0.9843 - 5s/epoch - 10ms/step\n",
      "Epoch 14/15\n",
      "540/540 - 5s - loss: 0.0221 - accuracy: 0.9931 - val_loss: 0.0272 - val_accuracy: 0.9910 - 5s/epoch - 10ms/step\n",
      "Epoch 15/15\n",
      "540/540 - 5s - loss: 0.0193 - accuracy: 0.9942 - val_loss: 0.0281 - val_accuracy: 0.9912 - 5s/epoch - 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23d5c0ad7e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCH = 15\n",
    "model.fit(train_data, epochs=NUM_EPOCH, validation_data=(validation_inputs, validation_targets),verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111ce63",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f5a59c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0925 - accuracy: 0.9754\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5e963bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.09, Test accuracy: 97.54%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {:.2f}, Test accuracy: {:.2f}%'.format(test_loss, test_accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da12637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3.10-TF",
   "language": "python",
   "name": "py310-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
